{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":196262,"sourceType":"datasetVersion","datasetId":84803}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installations and Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:08:54.886945Z","iopub.execute_input":"2025-03-16T18:08:54.887616Z","iopub.status.idle":"2025-03-16T18:08:56.105708Z","shell.execute_reply.started":"2025-03-16T18:08:54.887566Z","shell.execute_reply":"2025-03-16T18:08:56.104652Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Dataset Creation","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/bank-note-authentication-uci-data/BankNote_Authentication.csv\"\ndata = pd.read_csv(DATA_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:08:56.107144Z","iopub.execute_input":"2025-03-16T18:08:56.107674Z","iopub.status.idle":"2025-03-16T18:08:56.130257Z","shell.execute_reply.started":"2025-03-16T18:08:56.107641Z","shell.execute_reply":"2025-03-16T18:08:56.129187Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# checking the data\ndata.info()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:08:56.131820Z","iopub.execute_input":"2025-03-16T18:08:56.132228Z","iopub.status.idle":"2025-03-16T18:08:56.166268Z","shell.execute_reply.started":"2025-03-16T18:08:56.132162Z","shell.execute_reply":"2025-03-16T18:08:56.165127Z"}},"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1372 entries, 0 to 1371\nData columns (total 5 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   variance  1372 non-null   float64\n 1   skewness  1372 non-null   float64\n 2   curtosis  1372 non-null   float64\n 3   entropy   1372 non-null   float64\n 4   class     1372 non-null   int64  \ndtypes: float64(4), int64(1)\nmemory usage: 53.7 KB\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:08:56.167466Z","iopub.execute_input":"2025-03-16T18:08:56.167761Z","iopub.status.idle":"2025-03-16T18:08:56.193235Z","shell.execute_reply.started":"2025-03-16T18:08:56.167738Z","shell.execute_reply":"2025-03-16T18:08:56.192313Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"   variance  skewness  curtosis  entropy  class\n0   3.62160    8.6661   -2.8073 -0.44699      0\n1   4.54590    8.1674   -2.4586 -1.46210      0\n2   3.86600   -2.6383    1.9242  0.10645      0\n3   3.45660    9.5228   -4.0112 -3.59440      0\n4   0.32924   -4.4552    4.5718 -0.98880      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>variance</th>\n      <th>skewness</th>\n      <th>curtosis</th>\n      <th>entropy</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3.62160</td>\n      <td>8.6661</td>\n      <td>-2.8073</td>\n      <td>-0.44699</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.54590</td>\n      <td>8.1674</td>\n      <td>-2.4586</td>\n      <td>-1.46210</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3.86600</td>\n      <td>-2.6383</td>\n      <td>1.9242</td>\n      <td>0.10645</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3.45660</td>\n      <td>9.5228</td>\n      <td>-4.0112</td>\n      <td>-3.59440</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.32924</td>\n      <td>-4.4552</td>\n      <td>4.5718</td>\n      <td>-0.98880</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# splitting features and classes\nX, y = data.iloc[:, :-1], data.iloc[:, -1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:08:56.194443Z","iopub.execute_input":"2025-03-16T18:08:56.194833Z","iopub.status.idle":"2025-03-16T18:08:56.203628Z","shell.execute_reply.started":"2025-03-16T18:08:56.194794Z","shell.execute_reply":"2025-03-16T18:08:56.202149Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# converting data into numpy arrays for comptational reasons\nX = X.to_numpy()\ny = y.to_numpy().reshape(-1, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:08:56.204614Z","iopub.execute_input":"2025-03-16T18:08:56.204892Z","iopub.status.idle":"2025-03-16T18:08:56.224900Z","shell.execute_reply.started":"2025-03-16T18:08:56.204870Z","shell.execute_reply":"2025-03-16T18:08:56.223834Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# splitting data to train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42, stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:08:56.225948Z","iopub.execute_input":"2025-03-16T18:08:56.226284Z","iopub.status.idle":"2025-03-16T18:08:56.254040Z","shell.execute_reply.started":"2025-03-16T18:08:56.226257Z","shell.execute_reply":"2025-03-16T18:08:56.252653Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"## Define Network Structure","metadata":{}},{"cell_type":"code","source":"class TwoLayerPerceptron():\n    def __init__(self, X_train, y_train, n_hidden, n_y, lr, activation_fn):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.n_feature = X_train.shape[1]\n        self.n_hidden = n_hidden\n        self.n_y = n_y\n        self.params = {}\n        self.cache = {}\n        self.grads = {}\n        self.lr = lr\n        self.activation_fn = activation_fn\n\n    def initialize_params(self):\n        np.random.seed(42)\n        W1 = np.random.randn(self.n_hidden, self.n_feature) * 0.01\n        b1 = np.zeros((self.n_hidden, 1))\n        W2 = np.random.randn(self.n_y, self.n_hidden) * 0.01\n        b2 = np.zeros((self.n_y, 1))\n\n        self.params = {\n            \"W1\" : W1,\n            \"b1\" : b1,\n            \"W2\" : W2,\n            \"b2\" : b2\n        }\n        return self.params\n\n    def forward(self, X):\n        W1 = self.params[\"W1\"]\n        b1 = self.params[\"b1\"]\n        W2 = self.params[\"W2\"]\n        b2 = self.params[\"b2\"]\n\n        Z1 = np.dot(W1, X.T) + b1\n        if (self.activation_fn == \"tanh\"):\n            A1 = np.tanh(Z1)\n        elif (self.activation_fn == \"RELU\"):\n            A1 = self.RELU(Z1)\n            A1 = np.clip(A1, -0.9999, 0.9999)  \n        else:\n            print(\"Please write a valid activation function!\")\n        Z2 = np.dot(W2, A1) + b2\n        A2 = self.sigmoid(Z2)\n        A2 = np.clip(A2, 1e-10, 1 - 1e-10)\n\n        self.cache = {\n            \"Z1\" : Z1,\n            \"A1\" : A1,\n            \"Z2\" : Z2,\n            \"A2\" : A2,\n        }\n        return A2, self.cache\n\n    def loss(self):\n        A2 = self.cache[\"A2\"]\n        m = A2.shape[1]\n        Y = self.y_train\n        loss = - (np.dot(np.log(A2), Y) + np.dot(np.log(1 - A2), (1 - Y))) / m\n        loss = float(np.squeeze(loss))\n        return loss\n\n    def backward(self):\n        X = self.X_train\n        y = self.y_train\n        m = X.shape[0]\n        W1 = self.params[\"W1\"]\n        W2 = self.params[\"W2\"]\n        A1 = self.cache[\"A1\"]\n        A2 = self.cache[\"A2\"]\n\n        dZ2 = A2.T - y \n        dW2 = np.dot(dZ2.T, A1.T) / m \n        db2 = np.sum(dZ2, axis=0, keepdims=True) / m \n        dZ1 = np.dot(dZ2, W2) * (1 - np.power(A1, 2)).T \n        dW1 = np.dot(dZ1.T, X) / m \n        db1 = np.sum(dZ1, axis=0, keepdims=True) / m \n        \n        self.grads = {\n            \"dW1\" : dW1,\n            \"dW2\" : dW2,\n            \"db1\" : db1,\n            \"db2\" : db2\n        }\n\n        return self.grads\n\n    def update_params(self):\n        lr = self.lr\n        W1 = self.params[\"W1\"]\n        b1 = self.params[\"b1\"]\n        W2 = self.params[\"W2\"]\n        b2 = self.params[\"b2\"]\n\n        dW1 = self.grads[\"dW1\"]\n        db1 = self.grads[\"db1\"]\n        dW2 = self.grads[\"dW2\"]\n        db2 = self.grads[\"db2\"]\n\n        self.params[\"W1\"] -= self.lr * self.grads[\"dW1\"]\n        self.params[\"b1\"] -= self.lr * self.grads[\"db1\"].T\n        self.params[\"W2\"] -= self.lr * self.grads[\"dW2\"]\n        self.params[\"b2\"] -= self.lr * self.grads[\"db2\"]\n\n        return self.params\n\n    def train(self, num_steps, print_cost=True):\n        self.initialize_params()\n        X = self.X_train\n\n        for i in range(num_steps):\n            A2, cache = self.forward(X)\n            loss = self.loss()\n            grads = self.backward()\n            self.update_params()\n\n            if print_cost and i % 500 == 0:\n                print(f\"Loss at iteration {i} is {loss:.6f}\")\n        print(f\"Loss at iteration {num_steps} is {loss:.6f}\")\n\n    def predict(self, X_test):\n        params = self.params\n        A2, cache = self.forward(X_test)\n        preds = A2 > 0.5\n        return preds\n    \n    # helper functions\n    def sigmoid(self, Z):\n        Z = np.clip(Z, -500, 500)  # Prevent extreme values\n        return 1 / (1 + np.exp(-Z))\n\n    def RELU(self, x):\n        x = np.nan_to_num(x)  # Replace NaNs with 0\n        return np.maximum(0, x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:08:56.257370Z","iopub.execute_input":"2025-03-16T18:08:56.257736Z","iopub.status.idle":"2025-03-16T18:08:56.280407Z","shell.execute_reply.started":"2025-03-16T18:08:56.257694Z","shell.execute_reply":"2025-03-16T18:08:56.279164Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Training the Models","metadata":{}},{"cell_type":"code","source":"num_hidden = 5\nnum_y = 1\nlr = 1e-2\nMLP_tanh = TwoLayerPerceptron(X_train, y_train, num_hidden, num_y, lr, activation_fn=\"tanh\")\nMLP_RELU = TwoLayerPerceptron(X_train, y_train, num_hidden, num_y, lr, activation_fn=\"RELU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:08:56.282380Z","iopub.execute_input":"2025-03-16T18:08:56.282725Z","iopub.status.idle":"2025-03-16T18:08:56.305516Z","shell.execute_reply.started":"2025-03-16T18:08:56.282685Z","shell.execute_reply":"2025-03-16T18:08:56.304451Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"num_steps = 1500\nMLP_tanh.train(num_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:08:56.306402Z","iopub.execute_input":"2025-03-16T18:08:56.306710Z","iopub.status.idle":"2025-03-16T18:08:57.085350Z","shell.execute_reply.started":"2025-03-16T18:08:56.306684Z","shell.execute_reply":"2025-03-16T18:08:57.083981Z"}},"outputs":[{"name":"stdout","text":"Loss at iteration 0 is 0.693481\nLoss at iteration 500 is 0.293852\nLoss at iteration 1000 is 0.107952\nLoss at iteration 1500 is 0.065998\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"num_steps = 7500\nMLP_RELU.train(num_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:08:57.086506Z","iopub.execute_input":"2025-03-16T18:08:57.086907Z","iopub.status.idle":"2025-03-16T18:09:01.042033Z","shell.execute_reply.started":"2025-03-16T18:08:57.086868Z","shell.execute_reply":"2025-03-16T18:09:01.041035Z"}},"outputs":[{"name":"stdout","text":"Loss at iteration 0 is 0.693206\nLoss at iteration 500 is 0.547947\nLoss at iteration 1000 is 0.387134\nLoss at iteration 1500 is 0.324073\nLoss at iteration 2000 is 0.291502\nLoss at iteration 2500 is 0.273908\nLoss at iteration 3000 is 0.263561\nLoss at iteration 3500 is 0.256947\nLoss at iteration 4000 is 0.251553\nLoss at iteration 4500 is 0.246964\nLoss at iteration 5000 is 0.242369\nLoss at iteration 5500 is 0.238739\nLoss at iteration 6000 is 0.235589\nLoss at iteration 6500 is 0.232700\nLoss at iteration 7000 is 0.230127\nLoss at iteration 7500 is 0.228058\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"tanh_preds = MLP_tanh.predict(X_test)\nRELU_preds = MLP_RELU.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:09:01.044063Z","iopub.execute_input":"2025-03-16T18:09:01.044523Z","iopub.status.idle":"2025-03-16T18:09:01.050515Z","shell.execute_reply.started":"2025-03-16T18:09:01.044482Z","shell.execute_reply":"2025-03-16T18:09:01.049326Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"## Testing the Models","metadata":{}},{"cell_type":"code","source":"def print_metrics(y_test, y_pred):\n    y_test = y_test.flatten()\n    y_pred = y_pred.flatten()\n\n    acc = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average=\"binary\")\n    recall = recall_score(y_test, y_pred, average=\"binary\")\n    f1 = f1_score(y_test, y_pred, average=\"binary\")\n    conf_mx = confusion_matrix(y_test, y_pred)\n\n    print(f\"Accuracy: {acc:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(\"\\nConfusion Matrix:\")\n    print(conf_mx)\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:09:01.051578Z","iopub.execute_input":"2025-03-16T18:09:01.051921Z","iopub.status.idle":"2025-03-16T18:09:01.068942Z","shell.execute_reply.started":"2025-03-16T18:09:01.051892Z","shell.execute_reply":"2025-03-16T18:09:01.067763Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"print_metrics(y_test, tanh_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:09:01.070021Z","iopub.execute_input":"2025-03-16T18:09:01.070348Z","iopub.status.idle":"2025-03-16T18:09:01.104193Z","shell.execute_reply.started":"2025-03-16T18:09:01.070321Z","shell.execute_reply":"2025-03-16T18:09:01.103067Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.9855\nPrecision: 0.9683\nRecall: 1.0000\nF1 Score: 0.9839\n\nConfusion Matrix:\n[[149   4]\n [  0 122]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      0.97      0.99       153\n           1       0.97      1.00      0.98       122\n\n    accuracy                           0.99       275\n   macro avg       0.98      0.99      0.99       275\nweighted avg       0.99      0.99      0.99       275\n\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print_metrics(y_test, RELU_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-16T18:09:01.105103Z","iopub.execute_input":"2025-03-16T18:09:01.105539Z","iopub.status.idle":"2025-03-16T18:09:01.125983Z","shell.execute_reply.started":"2025-03-16T18:09:01.105510Z","shell.execute_reply":"2025-03-16T18:09:01.125050Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.9345\nPrecision: 0.9194\nRecall: 0.9344\nF1 Score: 0.9268\n\nConfusion Matrix:\n[[143  10]\n [  8 114]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.95      0.93      0.94       153\n           1       0.92      0.93      0.93       122\n\n    accuracy                           0.93       275\n   macro avg       0.93      0.93      0.93       275\nweighted avg       0.93      0.93      0.93       275\n\n","output_type":"stream"}],"execution_count":15}]}