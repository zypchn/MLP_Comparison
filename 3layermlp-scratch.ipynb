{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":196262,"sourceType":"datasetVersion","datasetId":84803}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Installations and Imports","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:51:04.043644Z","iopub.execute_input":"2025-03-18T12:51:04.043888Z","iopub.status.idle":"2025-03-18T12:51:06.119509Z","shell.execute_reply.started":"2025-03-18T12:51:04.043864Z","shell.execute_reply":"2025-03-18T12:51:06.118465Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"DATA_DIR = \"/kaggle/input/bank-note-authentication-uci-data/BankNote_Authentication.csv\"\ndata = pd.read_csv(DATA_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:51:36.379155Z","iopub.execute_input":"2025-03-18T12:51:36.379599Z","iopub.status.idle":"2025-03-18T12:51:36.401618Z","shell.execute_reply.started":"2025-03-18T12:51:36.379563Z","shell.execute_reply":"2025-03-18T12:51:36.400393Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"Data was inspected on a previous notebook, so we will skip that step.","metadata":{}},{"cell_type":"code","source":"# splitting features and classes\nX, y = data.iloc[:, :-1], data.iloc[:, -1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:51:38.444805Z","iopub.execute_input":"2025-03-18T12:51:38.445390Z","iopub.status.idle":"2025-03-18T12:51:38.454259Z","shell.execute_reply.started":"2025-03-18T12:51:38.445345Z","shell.execute_reply":"2025-03-18T12:51:38.453053Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# converting data to numpy arrays for comptational reasons\nX = X.to_numpy()\ny = y.to_numpy().reshape(-1, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:51:38.754843Z","iopub.execute_input":"2025-03-18T12:51:38.755258Z","iopub.status.idle":"2025-03-18T12:51:38.760516Z","shell.execute_reply.started":"2025-03-18T12:51:38.755218Z","shell.execute_reply":"2025-03-18T12:51:38.759483Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# splitting data to train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=42, stratify=y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:51:39.277854Z","iopub.execute_input":"2025-03-18T12:51:39.278175Z","iopub.status.idle":"2025-03-18T12:51:39.294086Z","shell.execute_reply.started":"2025-03-18T12:51:39.278149Z","shell.execute_reply":"2025-03-18T12:51:39.292874Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Define Network Structure","metadata":{}},{"cell_type":"code","source":"class ThreeLayerPerceptron():\n    def __init__(self, X_train, y_train, n_hidden, n_y, lr, activation_fn):\n        self.X_train = X_train\n        self.y_train = y_train\n        self.n_feature = X_train.shape[1]\n        self.n_hidden = n_hidden\n        self.n_y = n_y\n        self.params = {}\n        self.cache = {}\n        self.grads = {}\n        self.lr = lr\n        self.activation_fn = activation_fn\n\n    def initialize_params(self):\n        np.random.seed(42)\n        W1 = np.random.randn(self.n_hidden, self.n_feature) * 0.01\n        b1 = np.zeros((self.n_hidden, 1))\n        W2 = np.random.randn(self.n_hidden, self.n_hidden) * 0.01\n        b2 = np.zeros((self.n_hidden, 1))\n        W3 = np.random.randn(self.n_y, self.n_hidden) * 0.01\n        b3 = np.zeros((self.n_y, 1))\n\n        self.params = {\n            \"W1\" : W1,\n            \"b1\" : b1,\n            \"W2\" : W2,\n            \"b2\" : b2,\n            \"W3\" : W3,\n            \"b3\" : b3,\n        }\n        return self.params\n\n    def forward(self, X):\n        W1 = self.params[\"W1\"]\n        b1 = self.params[\"b1\"]\n        W2 = self.params[\"W2\"]\n        b2 = self.params[\"b2\"]\n        W3 = self.params[\"W3\"]\n        b3 = self.params[\"b3\"]\n        \n        if (self.activation_fn == \"tanh\"):\n            Z1 = np.dot(W1, X.T) + b1\n            A1 = np.tanh(Z1)\n            Z2 = np.dot(W2, A1) + b2\n            A2 = np.tanh(Z2)\n            Z3 = np.dot(W3, A2) + b3\n            A3 = self.sigmoid(Z3)\n            A3 = np.clip(A3, 1e-10, 1 - 1e-10)\n            \n        elif (self.activation_fn == \"RELU\"):\n            Z1 = np.dot(W1, X.T) + b1\n            A1 = self.RELU(Z1)\n            A1 = np.clip(A1, -0.9999, 0.9999)  \n            Z2 = np.dot(W2, A1) + b2\n            A2 = self.RELU(Z2)\n            A2 = np.clip(A2, -0.9999, 0.9999)\n            Z3 = np.dot(W3, A2) + b3\n            A3 = self.sigmoid(Z3)\n            A3 = np.clip(A3, 1e-10, 1 - 1e-10)\n            \n        else:\n            print(\"Please write a valid activation function!\")\n\n        self.cache = {\n            \"Z1\" : Z1,\n            \"A1\" : A1,\n            \"Z2\" : Z2,\n            \"A2\" : A2,\n            \"Z3\" : Z3,\n            \"A3\" : A3,\n        }\n        return A3, self.cache\n\n    def loss(self):\n        A3 = self.cache[\"A3\"]\n        m = A3.shape[1]\n        Y = self.y_train\n        loss = - (np.dot(np.log(A3), Y) + np.dot(np.log(1 - A3), (1 - Y))) / m\n        loss = float(np.squeeze(loss))\n        return loss\n\n    def backward(self):\n        X = self.X_train\n        y = self.y_train\n        m = X.shape[0]\n        W1 = self.params[\"W1\"]\n        W2 = self.params[\"W2\"]\n        W3 = self.params[\"W3\"]\n        A1 = self.cache[\"A1\"]\n        A2 = self.cache[\"A2\"]\n        A3 = self.cache[\"A3\"]\n\n        dZ3 = A3.T - y\n        dW3 = np.dot(dZ3.T, A2.T) / m\n        db3 = np.sum(dZ3, axis=0, keepdims=True)\n        \n        #dZ2 = A2.T - y \n        dZ2 = np.dot(dZ3, W3) * (1 - np.power(A2, 2)).T\n        dW2 = np.dot(dZ2.T, A1.T) / m \n        db2 = np.sum(dZ2, axis=0, keepdims=True) / m \n        \n        dZ1 = np.dot(dZ2, W2) * (1 - np.power(A1, 2)).T \n        dW1 = np.dot(dZ1.T, X) / m \n        db1 = np.sum(dZ1, axis=0, keepdims=True) / m \n        \n        self.grads = {\n            \"dW1\" : dW1,\n            \"dW2\" : dW2,\n            \"dW3\" : dW3,\n            \"db1\" : db1,\n            \"db2\" : db2,\n            \"db3\" : db3,\n        }\n\n        return self.grads\n\n    def update_params(self):\n        lr = self.lr\n        W1 = self.params[\"W1\"]\n        b1 = self.params[\"b1\"]\n        W2 = self.params[\"W2\"]\n        b2 = self.params[\"b2\"]\n        W3 = self.params[\"W3\"]\n        b3 = self.params[\"b3\"]\n\n        dW1 = self.grads[\"dW1\"]\n        db1 = self.grads[\"db1\"]\n        dW2 = self.grads[\"dW2\"]\n        db2 = self.grads[\"db2\"]\n        dW3 = self.grads[\"dW3\"]\n        db3 = self.grads[\"db3\"]\n\n        self.params[\"W1\"] -= self.lr * self.grads[\"dW1\"]\n        self.params[\"b1\"] -= self.lr * self.grads[\"db1\"].T\n        self.params[\"W2\"] -= self.lr * self.grads[\"dW2\"]\n        self.params[\"b2\"] -= self.lr * self.grads[\"db2\"].T\n        self.params[\"W3\"] -= self.lr * self.grads[\"dW3\"]\n        self.params[\"b3\"] -= self.lr * self.grads[\"db3\"]\n\n        return self.params\n\n    def train(self, num_steps, print_cost=True):\n        self.initialize_params()\n        X = self.X_train\n\n        for i in range(num_steps):\n            A3, cache = self.forward(X)\n            loss = self.loss()\n            grads = self.backward()\n            self.update_params()\n\n            if (loss < 0.20):\n                print(f\"Loss at iteration {i} is {loss:.6f}\")\n                return\n\n            if (print_cost and i % 500 == 0):\n                print(f\"Loss at iteration {i} is {loss:.6f}\")\n        print(f\"The model could not exceed the 0.2 threshold.\")\n        print(f\"Loss at iteration {i} is {loss:.6f}\")\n\n    def predict(self, X_test):\n        params = self.params\n        A3, cache = self.forward(X_test)\n        preds = A3 > 0.5\n        return preds\n    \n    # helper functions\n    def sigmoid(self, Z):\n        Z = np.clip(Z, -500, 500)  \n        return 1 / (1 + np.exp(-Z))\n\n    def RELU(self, x):\n        x = np.nan_to_num(x)  \n        return np.maximum(0, x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:51:46.047701Z","iopub.execute_input":"2025-03-18T12:51:46.048039Z","iopub.status.idle":"2025-03-18T12:51:46.070038Z","shell.execute_reply.started":"2025-03-18T12:51:46.048007Z","shell.execute_reply":"2025-03-18T12:51:46.068781Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"## Training the Models","metadata":{}},{"cell_type":"code","source":"num_hidden = 5\nnum_y = 1\nlr = 1e-2\nMLP_tanh = ThreeLayerPerceptron(X_train, y_train, num_hidden, num_y, lr, activation_fn=\"tanh\")\nMLP_relu = ThreeLayerPerceptron(X_train, y_train, num_hidden, num_y, lr, activation_fn=\"RELU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:51:53.782832Z","iopub.execute_input":"2025-03-18T12:51:53.783159Z","iopub.status.idle":"2025-03-18T12:51:53.787770Z","shell.execute_reply.started":"2025-03-18T12:51:53.783133Z","shell.execute_reply":"2025-03-18T12:51:53.786797Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"num_steps = 5000\nMLP_tanh.train(num_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:51:55.473115Z","iopub.execute_input":"2025-03-18T12:51:55.473541Z","iopub.status.idle":"2025-03-18T12:51:58.250032Z","shell.execute_reply.started":"2025-03-18T12:51:55.473481Z","shell.execute_reply":"2025-03-18T12:51:58.249043Z"}},"outputs":[{"name":"stdout","text":"Loss at iteration 0 is 0.693142\nLoss at iteration 500 is 1.041981\nLoss at iteration 1000 is 1.041966\nLoss at iteration 1500 is 1.041930\nLoss at iteration 2000 is 1.041816\nLoss at iteration 2500 is 1.041233\nLoss at iteration 3000 is 1.033466\nLoss at iteration 3500 is 0.444413\nLoss at iteration 3644 is 0.199680\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"num_steps = 10000\nMLP_relu.train(num_steps)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:52:58.537482Z","iopub.execute_input":"2025-03-18T12:52:58.537857Z","iopub.status.idle":"2025-03-18T12:53:06.957786Z","shell.execute_reply.started":"2025-03-18T12:52:58.537828Z","shell.execute_reply":"2025-03-18T12:53:06.956892Z"}},"outputs":[{"name":"stdout","text":"Loss at iteration 0 is 0.693146\nLoss at iteration 500 is 1.041991\nLoss at iteration 1000 is 1.041989\nLoss at iteration 1500 is 1.041987\nLoss at iteration 2000 is 1.041982\nLoss at iteration 2500 is 1.041975\nLoss at iteration 3000 is 1.041961\nLoss at iteration 3500 is 1.041935\nLoss at iteration 4000 is 1.041875\nLoss at iteration 4500 is 1.041713\nLoss at iteration 5000 is 1.041147\nLoss at iteration 5500 is 1.038629\nLoss at iteration 6000 is 1.023669\nLoss at iteration 6500 is 0.940988\nLoss at iteration 7000 is 0.661084\nLoss at iteration 7500 is 0.366599\nLoss at iteration 8000 is 0.319855\nLoss at iteration 8500 is 0.292973\nLoss at iteration 9000 is 0.276791\nLoss at iteration 9500 is 0.266534\nThe model could not exceed the 0.2 threshold.\nLoss at iteration 9999 is 0.258621\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Testing the Models","metadata":{}},{"cell_type":"code","source":"tanh_preds = MLP_tanh.predict(X_test)\nrelu_preds = MLP_relu.predict(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:53:11.983108Z","iopub.execute_input":"2025-03-18T12:53:11.983431Z","iopub.status.idle":"2025-03-18T12:53:11.988959Z","shell.execute_reply.started":"2025-03-18T12:53:11.983407Z","shell.execute_reply":"2025-03-18T12:53:11.987784Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def print_metrics(y_test, y_pred):\n    y_test = y_test.flatten()\n    y_pred = y_pred.flatten()\n\n    acc = accuracy_score(y_test, y_pred)\n    precision = precision_score(y_test, y_pred, average=\"binary\")\n    recall = recall_score(y_test, y_pred, average=\"binary\")\n    f1 = f1_score(y_test, y_pred, average=\"binary\")\n    conf_mx = confusion_matrix(y_test, y_pred)\n\n    print(f\"Accuracy: {acc:.4f}\")\n    print(f\"Precision: {precision:.4f}\")\n    print(f\"Recall: {recall:.4f}\")\n    print(f\"F1 Score: {f1:.4f}\")\n    print(\"\\nConfusion Matrix:\")\n    print(conf_mx)\n    print(\"\\nClassification Report:\")\n    print(classification_report(y_test, y_pred)) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:53:14.607849Z","iopub.execute_input":"2025-03-18T12:53:14.608209Z","iopub.status.idle":"2025-03-18T12:53:14.613940Z","shell.execute_reply.started":"2025-03-18T12:53:14.608175Z","shell.execute_reply":"2025-03-18T12:53:14.612936Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"print_metrics(y_test, tanh_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:53:16.839940Z","iopub.execute_input":"2025-03-18T12:53:16.840297Z","iopub.status.idle":"2025-03-18T12:53:16.861820Z","shell.execute_reply.started":"2025-03-18T12:53:16.840266Z","shell.execute_reply":"2025-03-18T12:53:16.860907Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.9745\nPrecision: 0.9675\nRecall: 0.9754\nF1 Score: 0.9714\n\nConfusion Matrix:\n[[149   4]\n [  3 119]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.98      0.97      0.98       153\n           1       0.97      0.98      0.97       122\n\n    accuracy                           0.97       275\n   macro avg       0.97      0.97      0.97       275\nweighted avg       0.97      0.97      0.97       275\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"print_metrics(y_test, relu_preds)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T12:53:19.574752Z","iopub.execute_input":"2025-03-18T12:53:19.575085Z","iopub.status.idle":"2025-03-18T12:53:19.595387Z","shell.execute_reply.started":"2025-03-18T12:53:19.575059Z","shell.execute_reply":"2025-03-18T12:53:19.594582Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.9200\nPrecision: 0.8968\nRecall: 0.9262\nF1 Score: 0.9113\n\nConfusion Matrix:\n[[140  13]\n [  9 113]]\n\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.94      0.92      0.93       153\n           1       0.90      0.93      0.91       122\n\n    accuracy                           0.92       275\n   macro avg       0.92      0.92      0.92       275\nweighted avg       0.92      0.92      0.92       275\n\n","output_type":"stream"}],"execution_count":14}]}